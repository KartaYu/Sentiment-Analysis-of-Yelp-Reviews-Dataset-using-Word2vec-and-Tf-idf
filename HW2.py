# -*- coding: utf-8 -*-
"""HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z7ae6pv4ybQQcaPrOnyMjkKT1lWaHryY
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer
import re
import nltk
from nltk.corpus import stopwords
import sys
from gensim.models import Word2Vec
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score
import multiprocessing

pd.set_option('display.max_rows', 10000)
nltk.download('stopwords')

def K_fold_CV(k, x, y, n):
  sum_acc = 0
  sub_size = int(y.shape[0]/k)

  if(n == 0):
    n = 200
  else:
    n = 10

  for i in range(0, k):
    x_train = np.delete(x, range(sub_size*i, sub_size*(i+1)), axis=0) # 切割train data
    print(str(sub_size*(i+1)) + 'finish')
    y_train = y.drop(df.index[sub_size*i:sub_size*(i+1)], axis=0) # 切割test data
    x_test = x[sub_size*i:sub_size*(i+1)] # 切割test data
    y_test = y[sub_size*i:sub_size*(i+1)] # 切割test data

    clf = RandomForestClassifier(min_samples_split=n, n_jobs=-1)
    clf.fit(x_train, y_train)
    acc_i = clf.score(x_test, y_test)
    sum_acc = acc_i + sum_acc
  
  
  return (sum_acc/4).round(4)

df = pd.read_csv("yelp.csv")
df = df.loc[:, ['stars', 'text']] # 資料保留
print(df)

df.loc[(df['stars'] <  4), 'stars'] = 0 # map 小於 4 to 0
df.loc[(df['stars'] >=  4), 'stars'] = 1 # map 大於等於 4 to 1
print(df)

vectorizer = CountVectorizer(stop_words=stopwords.words('english')) # 設定countvectorizer 
vect_trans = vectorizer.fit_transform(df['text']) # 去除stop_words
tfid = TfidfTransformer(smooth_idf=True).fit_transform(vect_trans) # tfid 轉換
result = pd.DataFrame(tfid.toarray(), columns=vectorizer.get_feature_names())
print(result)

for i in range(0, df.shape[0]):# df.shape[0]
  temp = []
  temp = df.iloc[i, 1]
  temp = re.split('\n | (?<!\n)[,.!?-](?!\n)', temp)
  
  if(i != 6450 and i != 4851): # 第4851與6450筆資料會造成資料轉換錯誤，因此跳過
    vectorizer.fit_transform(temp)
    feature = vectorizer.get_feature_names() # 萃取單字
    df['text'][i] = feature # 取代原有text資料內容
    # print(feature)

  """
  else:
    df['text'][i] = temp
  """
print(df)

max_cpu_counts = multiprocessing.cpu_count()
print(f"Use {max_cpu_counts} workers to train Word2Vec (dim={200})")
model = Word2Vec(df['text'], size=250, workers=8) # size 設定vector維度

# 下面為encoding
word_vec_endcoding = np.zeros((1, 250), dtype=float) # 為了有辦法append 先創一個都為0的array

for i in range(0, df.shape[0]): # 將各列的各自的字串丟入轉換成vector後取平均(合成該setence的vector)
  word_vec_sum = np.zeros((1, 250), dtype=float)
  
  for j in range(0, len(df['text'][i])):
    # word_vec_sum = model.wv.word_vec(df['text'][i][j]) + word_vec_sum
    if df['text'][i][j] in model.wv.vocab:
      word_vec_sum = model.wv.word_vec(df['text'][i][j]) + word_vec_sum

  word_vec_endcoding = np.append(word_vec_endcoding, np.divide(word_vec_sum, len(df['text'][i])), axis=0) # 計算平均
  
word_vec_endcoding = np.delete(word_vec_endcoding, 0, axis=0) # 刪掉最初都為0的array
result = pd.DataFrame(word_vec_endcoding)
print(result)

print("Accuracy of Word2vec : " + str(K_fold_CV(4, word_vec_endcoding, df['stars'], 0)))
print("Accuracy of tfid : " + str(K_fold_CV(4, tfid.toarray(), df['stars'], 1)))